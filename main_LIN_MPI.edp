// main_LIN_MPI.edp
//
// run with   mpirun -np [number_of_processes] FreeFEM++-mpi main_LIN_MPI.edp -o [offline_mode]
//            -o specifies the option for the offline stage: either "compute" (default) or "load"
// The user must ensure at all times that the appropriate mode is used for correct results
//
// Computation of a numerical approximation to the PDE defined in init.idp
// by the adv-MsFEM-LIN method
// Over-sampling is used if the parametervalue osCoef is larger than 0.5
//
// Parallelized version
//
// Parameters are read from parameters.txt
//
// 
// This file INCLUDES
// - init.idp to read parameters and initialize all objects needed for the MsFEM
// - local_problems_LIN.idp to execute local computations for the adv-MsFEM-LIN..
//   ..(numerical correctors and bubble functions) on a single coarse mesh element
// - tools_CR_and_OS.idp for local_problems_LIN_OS.idp to be included properly
// - local_problems_LIN_OS.idp to execute local computations for the adv-MsFEM-LIN..
//   ..on a single coarse mesh element with over-sampling
// - offline_effective_tensors_MPI.idp to compute the effective coefficients on a..
//   ..single coarse mesh element
// - offline_effective_tensors_MPI_reduce.idp to send all effective tensor..
//   ..computations to the main process
// - offline_effective_tensors_save.idp to save the effective coefficient
// - offline_load_MPI.idp to replace the offline phase if it can be preloaded
// - post.idp to create the output (reconstrucing the fine scale solution,.. 
//   ..saving the coarse solution, reconstructed solution, errors)
//
// This file CREATES for later use in the MsFEM:
//  uH -- the P1 solution to the effective macroscopic problem


// INITIALIZATION
string name = "Lin"; //accronyme for the parallelized adv-MsFEM-LIN
string nameMPI = "_MPI"; //added to name later, indicating usage of parallel code

// MPI
mpiComm comm(mpiCommWorld,0,0);
int nbproc = mpiSize(comm); //number of processes in parallel
int iproc = mpiRank(comm); //current process

include "msfem_blocks/init.idp"
// Build directories to save the results from the upcoming offline phase
if (iproc==0) {
    string createDir = "mkdir -p " + output;
    exec(createDir); //directory where all output is stored
    createDir = "mkdir -p " + basisDir;
    exec(createDir); //directory to store numerical correctors and bubble functions
    createDir = "mkdir -p " + coeffDir;
    exec(createDir); //directory to store the effective coefficients for the MsFEM
}
mpiBarrier(comm);

include "msfem_blocks/tools_CR_and_OS.idp" //load some functions and macros for OS


// OFFLINE PHASE
// Computation of numerical correctors and bubble functions per coarse mesh element..
// ..assigned to the current process
// VH0 is the P0 space on the coarse mesh -- created in init.idp
// Data structures storeVx, storeVy, store B were created in init.idp
int countOffline=0; //to count the number of triangles associated to the current process
if (offlineMode == "compute") {
    for(int i=0; i<VH0.ndof; i++) {
    if (iproc == i%nbproc) {
        cout <<"construction ms coefficients on tri "<<i<<endl;
        phi[][i]=2; //created in init.idp -- used to loop over the coarse mesh elements

        if (osCoef < 0.5) { //compute numerical correctors, bubble functions on K
            //osCoef is larger than 1 if over-sampling should be used
            include "msfem_blocks/local_problems_LIN.idp" 
            // Save (internally) the offline computations on K
            storeVx(countOffline,:)=Vx[]; storeVy(countOffline,:)=Vy[]; 
            if (useB) storeB(countOffline,:)=B[];
            // Compute effective coefficient on K
            include "msfem_blocks/offline_effective_tensors_MPI.idp"
        } else { //OS case
            include "msfem_blocks/local_problems_LIN_OS.idp" //compute numerical correctors, bubble functions on K
            // Save (internally) the offline computations on K
            storeVx(countOffline,:)=Vx[]; storeVy(countOffline,:)=Vy[]; 
            if (useB) storeB(countOffline,:)=B[];
            // Compute effective coefficient on K
            include "msfem_blocks/offline_effective_tensors_MPI.idp"
        }

        if (debug) cout << endl;
        phi[][i]=0;
        countOffline++;
    }
    }
    // send all information for the global problem to the main process
    include "msfem_blocks/offline_effective_tensors_MPI_reduce.idp"
    if (iproc==0) {//the main process saves the effective coefficient (for the entire coarse mesh)
        include "msfem_blocks/offline_effective_tensors_save.idp" 
    }
    mpiBarrier(comm);
} else if (offlineMode == "load") {
    include "msfem_blocks/offline_load_MPI.idp"
}


// ONLINE PHASE -- solving the effective problem
fespace VH(TH,P1); //coarse global FE space 
VH uH,vH; //the global problem is formulated on a standard FE P1-basis
if (iproc==0) { //only the main process solves
    solve PbGlob(uH,vH)= effectiveVar(uH,vH) + int1d(TH,qfe=qf1pE)(tgv*uH*vH);
    // BC need to be set depending on the type of FE used (conforming P1 here)
    // For P1 FE, the problem with classical Dirichlet conditions is well-posed,..
    // ..but these are not verified by OS numerical correctors and bubbles..
    // ..and at the interfaces, the reconstructed solution is discontinuous
    // effectiveVar is defined in init.idp in terms of the effective coefficients determined in the offline phase
    if (plots) plot(uH,value=1,wait=1,cmm="HMM solution");

    for (int i=1; i<nbproc; i++) { //send the coarse scale solution to the other processes
        Send(processor(i,comm), uH[]);
    }
}
if (iproc>0) Recv(processor(0,comm), uH[]); //the other processes receive the coarse scale solution

if (plots) plot(uH,value=1,wait=1,cmm="HMM solution");


// POST-PROCESSING -- reconstruction, error computation, documentation
include "msfem_blocks/post_MPI.idp"
