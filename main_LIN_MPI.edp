// main_LIN_MPI.edp
//
// run with   mpirun -np [number_of_processes] FreeFEM++-mpi main_LIN_MPI.edp -o [offline_mode]
//            -o specifies the option for the offline stage: either "compute" (default) or "load"
// The user must ensure at all times that the appropriate mode is used for correct results
//
// Computation of a numerical approximation to the PDE defined in init.idp
// by the adv-MsFEM-LIN method
// Over-sampling is used if the parametervalue osCoef is larger than 0.5
//
// Parallelized version
//
// Parameters are read from parameters.txt
//
// 
// This file INCLUDES
// - init.idp to read parameters and initialize all objects needed for the MsFEM
// - local_problems_LIN.idp to execute local computations for the adv-MsFEM-LIN..
//   ..(numerical correctors and bubble functions) on a single coarse mesh element
// - tools_CR_and_OS.idp for local_problems_LIN_OS.idp to be included properly
// - local_problems_LIN_OS.idp to execute local computations for the adv-MsFEM-LIN..
//   ..on a single coarse mesh element with oversampling
// - offline_effective_tensors_MPI.idp to compute the effective coefficients on a..
//   ..single coarse mesh element
// - offline_effective_tensors_MPI_reduce.idp to send all effective tensor..
//   ..computations to the main process
// - offline_save_effective_tensors.idp to save the effective coefficient
// - offline_load_MPI.idp to replace the offline phase if it can be preloaded
// - online.idp to construct and solve the (effective) macroscopic system
// - post.idp to create the output (reconstrucing the fine scale solution,.. 
//   ..saving the coarse solution, reconstructed solution, errors)
//
// This file CREATES for later use in the MsFEM:
//  bcType -- a string to set the correct boundary conditions in the macroscopic..
//            ..problem (Lin/CR) 
//  name[MPI] -- basic information about the MsFEM routine that is executed
//  VH -- the relevant P1 space for the macroscopic problem


// INITIALIZATION
string bcType = "Lin"; //type of boundary condtitions, relevant for the macroscopic problem
string name = bcType; //accronyme for the parallelized adv-MsFEM-LIN
string nameMPI = "_MPI"; //added to name later, indicating usage of parallel code

// MPI
mpiComm comm(mpiCommWorld,0,0);
int nbproc = mpiSize(comm); //number of processes in parallel
int iproc = mpiRank(comm); //current process

include "msfem_blocks/init.idp"
// Build directories to save the results from the upcoming offline phase
if (iproc==0) {
    string createDir = "mkdir -p " + output;
    exec(createDir); //directory where all output is stored
    createDir = "mkdir -p " + basisDir;
    exec(createDir); //directory to store numerical correctors and bubble functions
    createDir = "mkdir -p " + coeffDir;
    exec(createDir); //directory to store the effective coefficients for the MsFEM
}
mpiBarrier(comm);

include "msfem_blocks/tools_CR_and_OS.idp" //load some functions and macros for OS


// OFFLINE PHASE
// Computation of numerical correctors and bubble functions per coarse mesh element..
// ..assigned to the current process
// VH0 is the P0 space on the coarse mesh -- created in init.idp
// Data structures storeVx, storeVy, store B were created in init.idp
int countOffline=0; //to count the number of triangles associated to the current process
if (offlineMode == "compute") {
    for(int i=0; i<VH0.ndof; i++) {
    if (iproc == i%nbproc) {
        if (i%(2*n)==0) cout <<"construction ms coefficients on tri "<<i<<endl;
        phi[][i]=2; //created in init.idp -- used to loop over the coarse mesh elements

        if (osCoef < osThr) { //compute numerical correctors, bubble functions on K
            //osCoef is larger than osThr if oversampling should be used
            include "msfem_blocks/local_problems_LIN.idp" 
            // Save (internally) the offline computations on K
            storeVx(countOffline,:)=Vx[]; storeVy(countOffline,:)=Vy[]; 
            if (useB) storeB(countOffline,:)=B[];
            // Compute effective coefficient on K
            include "msfem_blocks/offline_effective_tensors_MPI.idp"
        } else { //OS case
            include "msfem_blocks/local_problems_LIN_OS.idp" //compute numerical correctors, bubble functions on K
            // Save (internally) the offline computations on K
            storeVx(countOffline,:)=Vx[]; storeVy(countOffline,:)=Vy[]; 
            if (useB) storeB(countOffline,:)=B[];
            // Compute effective coefficient on K
            include "msfem_blocks/offline_effective_tensors_MPI.idp"
        }

        if (debug) if (i%(2*n)==0) cout << endl;
        phi[][i]=0;
        countOffline++;
    }
    }
    // send all information for the global problem to the main process
    include "msfem_blocks/offline_effective_tensors_MPI_reduce.idp"
    if (iproc==0) {//the main process saves the effective coefficient (for the entire coarse mesh)
        include "msfem_blocks/offline_save_effective_tensors.idp" 
    }
    mpiBarrier(comm);
} else if (offlineMode == "load") {
    include "msfem_blocks/offline_load_MPI.idp"
}


// ONLINE PHASE -- solving the effective problem
fespace VH(TH,P1); //coarse global FE space 
include "msfem_blocks/online.idp"
// the P1 solution is (VH) uH, the bubble coefficients are in (VH0) uB

mpiBarrier(comm); //only the main process computes the solution
if (iproc==0) { 
    for (int i=1; i<nbproc; i++) { //send the coarse scale solution to the other processes
        Send(processor(i,comm), uH[]);
        Send(processor(i,comm), uB[]);
    }
}
//mpiBarrier(comm); //THIS BARRIER MUST NOT BE USED
if (iproc>0) {
    Recv(processor(0,comm), uH[]); //the other processes receive the coarse scale solution
    Recv(processor(0,comm), uB[]);
}

//if (plots) plot(uH,value=1,wait=1,cmm="HMM solution");


// POST-PROCESSING -- reconstruction, error computation, documentation
include "msfem_blocks/post_MPI.idp"
