// main_P1_LIN_MPI.edp
//
// run with   mpirun -np [number_of_processes] FreeFem++-mpi main_P1_LIN_MPI.edp -o [offline_mode]
//              (or a variant depending on local installation of mpi, FreeFem)
//            -o must be "compute" (default) or unspecified
//
////// Parallel version ////////////////////////////////////////////////

// Initialization
string FEtype = "P1"; //the vffile checks whether it is compatible with the intended FE type (P1 or multiscale)
string bcType = "Lin"; //type of boundary condtitions, relevant for the macroscopic problem
string name = bcType; //abbreviation used for the MsFEM output
string nameMPI = "_MPI"; //added to name later, indicating usage of parallel code
// MPI setup
mpiComm comm(mpiCommWorld,0,0);
int nbproc = mpiSize(comm); //number of processes in parallel
int iproc = mpiRank(comm); //current process

include "msfem_blocks/init.idp"
assert(strongDir==0);
assert(useB==0);
assert(testMS==0);
assert(offlineMode=="compute");
assert(osCoef < osThr);
mpiBarrier(comm); //all processes must wait for the main process to finish some actions (e.g. creating directories)

fespace VH(TH,P1); //coarse global FE conforming P1 space

// Offline stage
for(int i=0; i<VH0.ndof; i++) {
    int countOffline=0; //counter for the number of triangles treated in the offline stage on the current process
    if (iproc == i%nbproc) {
        if (i%(2*n)==0) {cout <<"construction ms coefficients on tri "<<i<<endl;}
        phi[][i]=2; //defined in init.idp -- used to loop over the coarse mesh elements
        mesh K=trunc(Th,phi>1,split=1,label=1); //fine mesh of the coarse mesh element K indicated by phi
        fespace VK(K,P1); //P1 finite element space on the triangle K
        VK Vc=0, Vx=0, Vy=0, B=0, uHx=x-xb[][i], uHy=y-yb[][i];
        VK phi0=1, phix=0, phiy=0; 
        // Compute effective coefficients on K
        offlineEffectiveTensors(K,i,phi0,phix,phiy,uHx,uHy,Vc,Vx,Vy,B)
        if (debug) if (i%(2*n)==0) cout << endl;
        phi[][i]=0;
        countOffline++;
    }
}
// Send all information for the global problem to the main process
offlineEffectiveTensorsReduceMPI
// Save effective coefficients (for the entire coarse mesh)
// Also save the Vc pattern over all coarse mesh elements
offlineSaveEffectiveTensors
mpiBarrier(comm);
if (iproc==0) printTime("Offline phase (computing + storing) lasted ")

// Online stage
VH uH=0;
VH0 uB=0; //introduced here only for compatibility with MsFEMs, that can use bubble functions
solveGlobalProblem(uH,uB)

mpiBarrier(comm); //only the main process computes the solution
if (iproc==0) { 
    for (int i=1; i<nbproc; i++) { //send the coarse scale solution to the other processes
        Send(processor(i,comm), uH[]);
    }
}
//mpiBarrier(comm); //THIS BARRIER MUST NOT BE USED
if (iproc>0) {
    Recv(processor(0,comm), uH[]); //the other processes receive the coarse scale solution
}

// Post-processing
postReconstructionErrorMPI(uH,uB) //error computation
postCoarseError(uH) //error computation of projections on the coarse space
