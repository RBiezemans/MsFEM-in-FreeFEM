// main_CR_MPI.edp
//
// run with   mpirun -np [number_of_processes] FreeFem++-mpi main_CR_MPI.edp -o [offline_mode]
//              (or a variant depending on local installation of mpi, FreeFem)
//            -o specifies the option for the offline stage: either "compute" (default) or "load"
// The user must ensure at all times that the appropriate mode is used for correct results
//
// Computation of a numerical approximation to the PDE defined in init.idp
// by the MsFEM-CR method based on the "vffile.idp" for local and global problems
// -- Over-sampling is used if the parametervalue osCoef is larger than osThr
// -- Bubble functions can be added to or omitted from the approximation space, and can be 
//    included in the linear system or carry the average of fRHS as coefficient.
//
//
// Parameters are read from parameters.txt
// Local and global variational forms are read from vffile.idp
//
// 
////// Parallelized version ////////////////////////////////////////////
//
//
// Global variables declared in this script ////////////////////////////
// - (string)  bcType -- a string to set the correct boundary conditions in the macroscopic problem (Lin/CR) 
// - (string)  name -- abbreviation for  MsFEM underlying affine space
// - (string)  nameMPI -- must be empty for sequential script; "_MPI" for parallel script
// - (int)     iproc -- index of the current process (always 0 for sequential script)
// - (int)     nbproc -- number of processes (always 1 for sequential script)
// - (int)     countOffline -- counter for the number of triangles associated to the current process
// - (fespace) VH -- the relevant P1 space for the macroscopic problem
///////////////////////////////////////////////////////////////////////


///////////////////////////////////////////////////////////////////////
// INITIALIZATION                                                    //
// (msfem_blocks/init.idp)                                           //
///////////////////////////////////////////////////////////////////////
string bcType = "CR"; //type of boundary condtitions, relevant for the macroscopic problem
string name = bcType; //abbreviation used for the MsFEM output
string nameMPI = "_MPI"; //added to name later, indicating usage of parallel code
// MPI
mpiComm comm(mpiCommWorld,0,0);
int nbproc = mpiSize(comm); //number of processes in parallel
int iproc = mpiRank(comm); //current process

include "msfem_blocks/init.idp"

mpiBarrier(comm);


///////////////////////////////////////////////////////////////////////
// OFFLINE STAGE                                                     //
// (msfem_blocks/local_problems_CR[_OS].idp                          //
//  or msfem_blocks/offline_load_MPI.idp)                            //
// (msfem_blocks/offline_effective_tensors_MPI.idp)                  //
// (msfem_blocks/offline_effective_tensors_MPI_reduce.idp)           //
// (msfem_blocks/offline_save_effective_tensors.idp)                 //
///////////////////////////////////////////////////////////////////////
// -- Computation of numerical correctors V[c/x/y] and bubble function B per coarse mesh element
// -- The discrete RHS is also computed via the inclusion of msfem_blocks/offline_effective_RHS.idp in the files for the offline stage
// -- Data structures storeVx, storeVy, store B are declared in init.idp
int countOffline=0; //to count the number of triangles associated to the current process
if (offlineMode == "compute") {
    for(int i=0; i<VH0.ndof; i++) {
    if (iproc == i%nbproc) {
        if (i%(2*n)==0) {
            cout <<"construction ms coefficients on tri "<<i<<endl;
            // if (iproc==0) ffLog <<"construction ms coefficients on tri "<<i<<endl;
        }
        phi[][i]=2; //initialized in init.idp -- used to loop over the coarse mesh elements

        if (osCoef < osThr) {//oversampling is used iff osCoef is larger than osThr
            //osCoef is larger than osThr if oversampling should be used
            include "msfem_blocks/local_problems_CR.idp"
            // Save (internally) the offline computations on K
            if (useVc) storeVc(countOffline,:)=Vc[];
            storeVx(countOffline,:)=Vx[]; storeVy(countOffline,:)=Vy[]; 
            if (useB) storeB(countOffline,:)=B[];
            // Compute effective coefficient on K
            include "msfem_blocks/offline_effective_tensors_MPI.idp"
        } else { //OS case
            include "msfem_blocks/local_problems_CR_OS.idp"
            // Save (internally) the offline computations on K
            if (useVc) storeVc(countOffline,:)=Vc[];
            storeVx(countOffline,:)=Vx[]; storeVy(countOffline,:)=Vy[]; 
            if (useB) storeB(countOffline,:)=B[];
            // Compute effective coefficient on K
            include "msfem_blocks/offline_effective_tensors_MPI.idp"
        }
        if (debug) if (i%(2*n)==0) cout << endl;
        phi[][i]=0;
        countOffline++;
    }
    }
    // send all information for the global problem to the main process
    include "msfem_blocks/offline_effective_tensors_MPI_reduce.idp"
    if (iproc==0) {//the main process saves the effective coefficient (for the entire coarse mesh)
        include "msfem_blocks/offline_save_effective_tensors.idp" 
    }
    mpiBarrier(comm);
    if (iproc==0) printTime("Offline phase (computing + storing) lasted ")
} 
else if (offlineMode == "load") 
{
    include "msfem_blocks/offline_load_MPI.idp"
}


///////////////////////////////////////////////////////////////////////
// ONLINE STAGE                                                      //
// (msfem_blocks/online.idp                                          //
///////////////////////////////////////////////////////////////////////
// -- Solving the effective problem  
fespace VH(TH,P1nc); //coarse global FE Crouzeix-Raviart space 
include "msfem_blocks/online.idp"
// the P1 solution is stored in (VH) uH, the bubble coefficients in (VH0) uB

mpiBarrier(comm); //only the main process computes the solution
if (iproc==0) { 
    for (int i=1; i<nbproc; i++) { //send the coarse scale solution to the other processes
        Send(processor(i,comm), uH[]);
        Send(processor(i,comm), uB[]);
    }
}
if (iproc>0) {
    Recv(processor(0,comm), uH[]); //the other processes receive the coarse scale solution
    Recv(processor(0,comm), uB[]);
}


///////////////////////////////////////////////////////////////////////
// POST-PROCESSING                                                   //
// (msfem_blocks/post_MPI.idp)                                       //
// (msfem_blocks/write_results_MPI.idp)                              //
///////////////////////////////////////////////////////////////////////
// -- Reconstruction, error computation, documentation
include "msfem_blocks/post_MPI.idp"
include "msfem_blocks/write_results_MPI.idp"
